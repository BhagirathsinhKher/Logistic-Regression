{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "$$ #Logistic Regression $$"
      ],
      "metadata": {
        "id": "WKL79IB2mfgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Theoritical Questions:"
      ],
      "metadata": {
        "id": "ScxAyeVNm1YM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "  \n",
        "- **Logistic Regression:**\n",
        "  - **Purpose**: Used for **classification problems**, especially **binary classification** (e.g., Yes/No, 0/1, True/False).\n",
        "  - **Output**: Predicts the **probability** of a class label (between 0 and 1).\n",
        "  - **Activation Function**: Uses the **sigmoid function** to map predictions to probabilities:\n",
        "  \n",
        "  $$\n",
        "  \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "  $$\n",
        "\n",
        "  where $z = w_1x_1 + w_2x_2 + \\ldots + w_nx_n + b$.\n",
        "\n",
        "- **Final Output**: Converts probability to class label using a threshold (typically 0.5).\n",
        "\n",
        "- **Linear Regression:**\n",
        "  - **Purpose**: Used for **regression problems** — predicting **continuous numeric values**.\n",
        "  - **Output**: Directly predicts a **real number** as output.\n",
        "  - **Equation**:\n",
        "\n",
        "  $$\n",
        "  y = w_1x_1 + w_2x_2 + \\ldots + w_nx_n + b\n",
        "  $$\n",
        "\n",
        "  - **Loss Function**: Uses **Mean Squared Error (MSE)**.\n",
        "  \n",
        "- **Key Differences:**\n",
        "\n",
        "| Feature                | Linear Regression                      | Logistic Regression                      |\n",
        "|------------------------|----------------------------------------|------------------------------------------|\n",
        "| Type of Problem        | Regression (continuous output)         | Classification (discrete output)         |\n",
        "| Output Range           | $(-\\infty, +\\infty)$                   | $(0, 1)$ (Probability)                   |\n",
        "| Function Used          | Linear Function                        | Sigmoid Function                         |\n",
        "| Loss Function          | Mean Squared Error                     | Binary Cross-Entropy (Log Loss)          |\n",
        "| Interpretation         | Predicts a value                       | Predicts probability of class            |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_w5EeC_cnH2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the mathematical equation of Logistic Regression?\n",
        "- **Mathematical Equation of Logistic Regression:**\n",
        "  - The logistic regression model predicts the **probability** that a given input $x$ belongs to the **positive class** (class 1).\n",
        "  - The equation is based on the **sigmoid (logistic) function** applied to a linear combination of inputs:\n",
        "\n",
        "  $$\n",
        "  P(y = 1 \\mid x) = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "  $$\n",
        "\n",
        "  where,\n",
        "\n",
        "  $$\n",
        "  z = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_nx_n = \\mathbf{w}^T \\mathbf{x}\n",
        "  $$\n",
        "\n",
        "  - Here:\n",
        "    - $\\mathbf{x} = [1, x_1, x_2, \\ldots, x_n]$ is the input vector (including bias term as 1)\n",
        "    - $\\mathbf{w} = [w_0, w_1, w_2, \\ldots, w_n]$ is the weight vector\n",
        "    - $\\sigma(z)$ is the **sigmoid function** which maps any real value to the range $(0, 1)$\n",
        "\n",
        "- **Final Class Prediction Rule:**\n",
        "\n",
        "  $$\n",
        "  \\hat{y} =\n",
        "  \\begin{cases}\n",
        "    1 & \\text{if } \\sigma(z) \\geq 0.5 \\\\\n",
        "    0 & \\text{if } \\sigma(z) < 0.5\n",
        "  \\end{cases}\n",
        "  $$\n",
        "\n",
        "- This makes Logistic Regression suitable for **binary classification** problems by outputting probabilities that can be thresholded into class labels.\n",
        "---"
      ],
      "metadata": {
        "id": "-SSBvu18m8et"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Why do we use the Sigmoid function in Logistic Regression?\n",
        "- **Purpose**: The sigmoid function transforms the linear output (which can range from $-\\infty$ to $+\\infty$) into a probability value between **0 and 1**, which is ideal for binary classification.\n",
        "\n",
        "- **Sigmoid Function Definition**:\n",
        "\n",
        "  $$\n",
        "  \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "  $$\n",
        "\n",
        "- **Key Reasons to Use Sigmoid:**\n",
        "  - **Probability Mapping**: Converts linear output to a value between **0 and 1**, allowing interpretation as a **probability**.\n",
        "  - **Smooth Gradient**: The function is **differentiable**, which is essential for optimization using **gradient descent**.\n",
        "  - **Thresholding**: We can apply a **decision threshold** (typically 0.5) to the output to classify observations into one of the two classes.\n",
        "  - **Mathematical Simplicity**: Its derivative has a clean form, which simplifies the computation of gradients:\n",
        "\n",
        "    $$\n",
        "    \\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\n",
        "    $$\n",
        "\n",
        "- **Conclusion**: The sigmoid function ensures that logistic regression outputs a **valid probability**, making it ideal for classification problems.\n",
        "---"
      ],
      "metadata": {
        "id": "-lzURa-ipLba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the cost function of Logistic Regression?\n",
        "- **Cost Function of Logistic Regression**: Measures how well the predicted probabilities match the actual class labels. It is based on the **likelihood function**, transformed using the **logarithm** to simplify optimization.\n",
        "\n",
        "- **Binary Cross-Entropy (Log Loss)** is used as the cost function:\n",
        "\n",
        "  $$\n",
        "  J(\\mathbf{w}) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]\n",
        "  $$\n",
        "\n",
        "  where:\n",
        "  - $m$ = number of training examples  \n",
        "  - $y^{(i)}$ = actual label (0 or 1) for the $i^{th}$ example  \n",
        "  - $\\hat{y}^{(i)} = \\sigma(z^{(i)})$ = predicted probability using the sigmoid function\n",
        "\n",
        "- **Intuition**:\n",
        "  - If the true label is 1, only the first term is active: $- \\log(\\hat{y})$\n",
        "  - If the true label is 0, only the second term is active: $- \\log(1 - \\hat{y})$\n",
        "  - The function penalizes confident wrong predictions heavily and rewards confident correct ones.\n",
        "\n",
        "- **Why this cost function?**\n",
        "  - It ensures **convexity**, making it easier to optimize using gradient descent.\n",
        "  - It effectively penalizes incorrect predictions more strongly as the confidence in the wrong prediction increases.\n",
        "---"
      ],
      "metadata": {
        "id": "bkxIU6wzpnHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "- **Regularization**: A technique used to **prevent overfitting** by adding a **penalty term** to the cost function. It discourages the model from fitting the noise in the training data by **shrinking the weights**.\n",
        "\n",
        "- **In Logistic Regression**, the regularized cost function becomes:\n",
        "\n",
        "  - **L2 Regularization (Ridge)** – adds the squared magnitude of weights:\n",
        "\n",
        "    $$\n",
        "    J(\\mathbf{w}) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2\n",
        "    $$\n",
        "\n",
        "  - **L1 Regularization (Lasso)** – adds the absolute values of weights:\n",
        "\n",
        "    $$\n",
        "    J(\\mathbf{w}) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right] + \\frac{\\lambda}{m} \\sum_{j=1}^{n} |w_j|\n",
        "    $$\n",
        "\n",
        "  where:\n",
        "  - $\\lambda$ = regularization parameter (controls the strength of the penalty)\n",
        "  - $w_j$ = weights of the model (excluding bias term)\n",
        "\n",
        "- **Why is Regularization Needed?**\n",
        "  - **Prevents Overfitting**: Helps avoid overly complex models that perform well on training data but poorly on unseen data.\n",
        "  - **Controls Model Complexity**: Keeps weights small, which simplifies the model and improves generalization.\n",
        "  - **Improves Generalization**: Leads to better performance on test/validation datasets.\n",
        "\n",
        "- **Conclusion**: Regularization helps create **simpler, more robust models** by penalizing large weights and reducing variance.\n",
        "---"
      ],
      "metadata": {
        "id": "TfBC-acMp1Dw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "- **Lasso Regression (L1 Regularization)**:\n",
        "  - **Penalty**: Adds the **absolute value** of the weights to the cost function.\n",
        "  - **Cost Function**:\n",
        "\n",
        "    $$\n",
        "    J(\\mathbf{w}) = \\text{Loss} + \\lambda \\sum_{j=1}^{n} |w_j|\n",
        "    $$\n",
        "\n",
        "  - **Effect**: Lasso can set some coefficients to **exactly zero**, leading to **sparse models** and performing **feature selection**. It works well when only a few features are important.\n",
        "  - **Use Case**: Ideal when you believe only a subset of features is useful for prediction.\n",
        "\n",
        "- **Ridge Regression (L2 Regularization)**:\n",
        "  - **Penalty**: Adds the **squared value** of the weights to the cost function.\n",
        "  - **Cost Function**:\n",
        "\n",
        "    $$\n",
        "    J(\\mathbf{w}) = \\text{Loss} + \\lambda \\sum_{j=1}^{n} w_j^2\n",
        "    $$\n",
        "\n",
        "  - **Effect**: Ridge **shrinks** the coefficients but **does not set them to zero**. It reduces model complexity and helps with multicollinearity.\n",
        "  - **Use Case**: Useful when most features are expected to be relevant but need regularization to prevent overfitting.\n",
        "\n",
        "- **Elastic Net Regression**:\n",
        "  - **Penalty**: Combines **both L1 and L2 penalties** to take advantage of both Lasso and Ridge regression.\n",
        "  - **Cost Function**:\n",
        "\n",
        "    $$\n",
        "    J(\\mathbf{w}) = \\text{Loss} + \\lambda_1 \\sum_{j=1}^{n} |w_j| + \\lambda_2 \\sum_{j=1}^{n} w_j^2\n",
        "    $$\n",
        "\n",
        "  - **Effect**: Elastic Net can perform **feature selection** like Lasso and **shrink coefficients** like Ridge. It is particularly useful when there are **many correlated features**.\n",
        "  - **Use Case**: Ideal when there are a large number of features, and you believe both regularization and feature selection are needed.\n",
        "\n",
        "- **Comparison Table**:\n",
        "\n",
        "| Feature                | Lasso Regression (L1)               | Ridge Regression (L2)               | Elastic Net Regression               |\n",
        "|------------------------|-------------------------------------|-------------------------------------|--------------------------------------|\n",
        "| **Penalty Type**        | Absolute values of coefficients     | Squared values of coefficients      | Combination of L1 and L2 penalties   |\n",
        "| **Effect on Coefficients**| Can set some coefficients to zero  | Shrinks coefficients but does not set them to zero | Shrinks coefficients and can set some to zero |\n",
        "| **Feature Selection**   | Yes, selects a subset of features  | No, keeps all features             | Yes, can select features while shrinking others |\n",
        "| **Use Case**            | Sparse models, when few features matter | Models with many features, multicollinearity | When features are highly correlated and both selection and shrinkage are needed |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "8-oGK4MYqITt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        "- **When Features are Highly Correlated**: Elastic Net is particularly useful when there are **many correlated features**. Lasso may select one feature from a group of correlated features and ignore the rest, while Ridge would shrink all of them equally. Elastic Net balances between Lasso's feature selection and Ridge's shrinkage, effectively handling correlated predictors.\n",
        "\n",
        "- **When You Need Both Feature Selection and Shrinkage**: If you need both **feature selection** (like Lasso) and **shrinkage** (like Ridge), Elastic Net is a good choice. This combination allows you to **select important features** and **shrink the coefficients** of the less important ones, making it more robust than Lasso or Ridge alone.\n",
        "\n",
        "- **When You Have Many Features (p > n)**: In cases where the number of features $p$ is greater than the number of samples $n$, Lasso and Ridge may not work as well. Elastic Net combines the advantages of both Lasso and Ridge, making it more effective in high-dimensional settings where feature selection and shrinkage are both required.\n",
        "\n",
        "- **When Lasso Fails (All Features are Relevant)**: If Lasso produces a **too sparse model** (with many coefficients set to zero), Elastic Net can help by allowing some correlation between features but still promoting regularization, avoiding the over-sparsity issue of Lasso.\n",
        "\n",
        "- **When Lasso and Ridge Do Not Perform Well Individually**: Elastic Net can be more effective when Lasso or Ridge does not yield satisfactory performance. It is especially useful in situations where:\n",
        "  - Lasso might be too aggressive in setting coefficients to zero.\n",
        "  - Ridge might fail to reduce variance enough in complex models.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9VrKFT_DwwOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "\n",
        "- **Impact of the Regularization Parameter (λ) in Logistic Regression**:\n",
        "\n",
        "  - **Control Overfitting**: The regularization parameter λ controls the strength of the regularization applied to the model. It prevents **overfitting** by penalizing large weights and simplifying the model.\n",
        "\n",
        "  - **When λ is Small**:\n",
        "    - The regularization effect is **weak**, and the model is allowed to fit the training data more closely.\n",
        "    - The weights may become **large**, which can lead to overfitting, especially in the presence of noise or irrelevant features.\n",
        "    - The model will likely have high variance and may not generalize well to unseen data.\n",
        "\n",
        "  - **When λ is Large**:\n",
        "    - The regularization effect becomes **stronger**, and the model is forced to keep the weights **small**.\n",
        "    - This results in a **simpler model**, which may have lower variance and reduced risk of overfitting.\n",
        "    - However, if λ is too large, the model may become too **simplified**, underfitting the data and failing to capture important patterns.\n",
        "\n",
        "  - **Optimal λ**:\n",
        "    - The optimal value of λ lies between **small** and **large** values. It balances the trade-off between bias and variance:\n",
        "      - A **small λ** (close to zero) leads to **under-regularization**, where the model is more complex and can overfit.\n",
        "      - A **large λ** leads to **over-regularization**, where the model becomes too simple and may underfit.\n",
        "    - The optimal λ is typically determined using **cross-validation**, where the model is trained on different subsets of the data, and its performance is evaluated to find the best value of λ.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4_VtTuseqhT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression?\n",
        "- **Key Assumptions of Logistic Regression**:\n",
        "\n",
        "  1. **Linear Relationship between Predictors and Log-Odds**:\n",
        "     - Logistic regression assumes that there is a **linear relationship** between the independent variables (predictors) and the **log-odds** of the dependent variable (the log of the odds of the event happening).\n",
        "     - The equation takes the form:\n",
        "\n",
        "$$\n",
        "       \\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n\n",
        "$$\n",
        "where $p$ is the probability of the event occurring.\n",
        "\n",
        "  2. **Independence of Observations**:\n",
        "     - Each observation in the dataset should be **independent** of others. Logistic regression assumes that the data points are not correlated, meaning the outcome of one observation does not influence another.\n",
        "\n",
        "  3. **No or Little Multicollinearity**:\n",
        "     - Logistic regression assumes that the predictors (independent variables) are not highly correlated with each other. If the independent variables are highly correlated, it can lead to multicollinearity, making it difficult to determine the individual effect of each predictor on the outcome.\n",
        "\n",
        "  4. **Large Sample Size**:\n",
        "     - Logistic regression works best with **large sample sizes**. Small sample sizes can lead to unstable estimates of the coefficients and inaccurate predictions.\n",
        "     - In general, a sample size of at least 10 events per predictor is recommended to avoid overfitting and unreliable results.\n",
        "\n",
        "  5. **No High Influence Points (Outliers)**:\n",
        "     - Logistic regression assumes that the data does not have influential outliers that significantly affect the model. Outliers can lead to biased estimates of the coefficients.\n",
        "     - Techniques like **robust regression** or **removal of outliers** may be needed if significant outliers are present.\n",
        "\n",
        "  6. **Binary Dependent Variable**:\n",
        "     - Logistic regression assumes that the dependent variable is **binary** (i.e., it can take on only two possible outcomes, such as 0 or 1, Yes or No).\n",
        "     - For multi-class problems, **multinomial logistic regression** or **one-vs-rest classification** methods are used.\n",
        "\n",
        "  7. **Homoscedasticity** (for Generalized Linear Models):\n",
        "     - The variance of the errors should be **constant** across all levels of the independent variables. While logistic regression is a **generalized linear model** and does not require the traditional homoscedasticity assumption, it is important that errors are **independent**.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "0LwQbY5pxKh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "- **Alternatives to Logistic Regression for Classification Tasks**:\n",
        "\n",
        "  1. **Decision Trees**:\n",
        "     - **Description**: A **tree-like model** that splits data based on feature values to classify outcomes.\n",
        "     - **Advantages**:\n",
        "       - Can handle both **categorical and numerical** data.\n",
        "       - Easy to interpret and visualize.\n",
        "     - **Disadvantages**:\n",
        "       - Prone to **overfitting** if the tree is too deep.\n",
        "       - Sensitive to small variations in data.\n",
        "\n",
        "  2. **Random Forest**:\n",
        "     - **Description**: An **ensemble method** that builds multiple decision trees and combines their predictions to improve accuracy.\n",
        "     - **Advantages**:\n",
        "       - **Robust** to overfitting compared to individual decision trees.\n",
        "       - Handles missing data and large feature sets well.\n",
        "     - **Disadvantages**:\n",
        "       - Less interpretable than a single decision tree.\n",
        "       - Can be computationally expensive.\n",
        "\n",
        "  3. **Support Vector Machines (SVM)**:\n",
        "     - **Description**: A **boundary-based method** that finds the optimal hyperplane to separate classes in high-dimensional space.\n",
        "     - **Advantages**:\n",
        "       - Effective in **high-dimensional** spaces.\n",
        "       - Can use different kernel functions for **non-linear** classification.\n",
        "     - **Disadvantages**:\n",
        "       - Requires careful tuning of the **kernel** and **hyperparameters**.\n",
        "       - Computationally expensive for large datasets.\n",
        "\n",
        "  4. **k-Nearest Neighbors (k-NN)**:\n",
        "     - **Description**: A **distance-based algorithm** where the class of a data point is determined by the majority class of its k nearest neighbors.\n",
        "     - **Advantages**:\n",
        "       - **Non-parametric** and simple to implement.\n",
        "       - No model training required.\n",
        "     - **Disadvantages**:\n",
        "       - **Slow** for large datasets (due to distance calculations).\n",
        "       - Performance depends heavily on the choice of **k** and distance metric.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "isiq3_VbxO8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are Classification Evaluation Metrics?\n",
        "\n",
        "- **Classification Evaluation Metrics**:\n",
        "\n",
        "  These metrics help assess the **performance** of a classification model. They are especially useful when comparing models or choosing the best one.\n",
        "\n",
        "- **1. Accuracy**:\n",
        "  - **Definition**: The ratio of correctly predicted observations to the total observations.\n",
        "  - **Formula**:\n",
        "    $$\n",
        "    \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "    $$\n",
        "  - **Use Case**: Good when classes are balanced.\n",
        "\n",
        "- **2. Precision**:\n",
        "  - **Definition**: Out of all predicted positives, how many were actually positive?\n",
        "  - **Formula**:\n",
        "    $$\n",
        "    \\text{Precision} = \\frac{TP}{TP + FP}\n",
        "    $$\n",
        "  - **Use Case**: Important when **false positives** are costly (e.g., spam detection).\n",
        "\n",
        "- **3. Recall (Sensitivity or True Positive Rate)**:\n",
        "  - **Definition**: Out of all actual positives, how many were correctly predicted?\n",
        "  - **Formula**:\n",
        "    $$\n",
        "    \\text{Recall} = \\frac{TP}{TP + FN}\n",
        "    $$\n",
        "  - **Use Case**: Important when **false negatives** are costly (e.g., disease detection).\n",
        "\n",
        "- **4. F1 Score**:\n",
        "  - **Definition**: The harmonic mean of Precision and Recall. It balances the two when there’s an uneven class distribution.\n",
        "  - **Formula**:\n",
        "    $$\n",
        "    \\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "    $$\n",
        "  - **Use Case**: Best when **precision and recall** are both important.\n",
        "\n",
        "- **5. Confusion Matrix**:\n",
        "  - **Definition**: A table that shows the number of correct and incorrect predictions broken down by each class.\n",
        "\n",
        "|                      | Predicted Positive | Predicted Negative |\n",
        "  |----------------------|--------------------|--------------------|\n",
        "  | **Actual Positive**  | True Positive (TP) | False Negative (FN)|\n",
        "  | **Actual Negative**  | False Positive (FP)| True Negative (TN) |\n",
        "\n",
        "- **6. ROC Curve (Receiver Operating Characteristic)**:\n",
        "  - **Definition**: Plots the True Positive Rate (Recall) against the False Positive Rate.\n",
        "  - **Use Case**: Helps visualize model performance across all classification thresholds.\n",
        "\n",
        "- **7. AUC (Area Under the ROC Curve)**:\n",
        "  - **Definition**: Measures the entire two-dimensional area underneath the ROC curve.\n",
        "  - **Interpretation**:\n",
        "    - AUC = 1 → Perfect model\n",
        "    - AUC = 0.5 → No discrimination (random)\n",
        "\n",
        "- **8. Log Loss (Logarithmic Loss)**:\n",
        "  - **Definition**: Measures the uncertainty of the probability predictions.\n",
        "  - **Formula**:\n",
        "  $$\n",
        "    - \\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)]\n",
        "  $$\n",
        "  - **Use Case**: Useful when you want to penalize **false confident predictions** more heavily.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "09i01-nSyqu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How does class imbalance affect Logistic Regression?\n",
        "\n",
        "- **Class Imbalance in Logistic Regression**:\n",
        "\n",
        "  Class imbalance occurs when one class (e.g., \"positive\") is significantly underrepresented compared to another (e.g., \"negative\"). This imbalance can lead to biased models.\n",
        "\n",
        "- **Impact on Logistic Regression**:\n",
        "  - **Biased Predictions**: The model may learn to always predict the majority class because it minimizes the loss function effectively that way.\n",
        "  - **Misleading Accuracy**: A high accuracy score can be deceptive. For instance, if 95% of data is class 0, the model can achieve 95% accuracy by always predicting 0 — but it fails to detect any minority class.\n",
        "  - **Poor Recall for Minority Class**: Logistic Regression might have **low recall** for the underrepresented class, missing important positive instances (e.g., detecting fraud or disease).\n",
        "  - **Unstable Thresholds**: Default threshold of 0.5 may not be optimal for imbalanced data, leading to further misclassification.\n",
        "\n",
        "- **Solutions**:\n",
        "  - **Resampling Techniques**:\n",
        "    - **Oversampling**: Increase the number of minority class samples (e.g., using SMOTE).\n",
        "    - **Undersampling**: Reduce the number of majority class samples.\n",
        "  - **Use Class Weights**:\n",
        "    - Logistic Regression in libraries like `scikit-learn` allows setting `class_weight='balanced'` to penalize misclassification of minority class more.\n",
        "  - **Alternative Metrics**: Use metrics like **Precision**, **Recall**, **F1 Score**, and **AUC** instead of Accuracy.\n",
        "  - **Adjusting Decision Threshold**: Modify the probability threshold (e.g., lower than 0.5) to favor detecting the minority class.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LnYlWPa9M2kb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "- **Hyperparameter Tuning in Logistic Regression**:\n",
        "\n",
        "  Hyperparameter tuning refers to the process of **finding the optimal values of the hyperparameters** that are not learned during model training but significantly affect the model's performance.\n",
        "\n",
        "- **Common Hyperparameters in Logistic Regression**:\n",
        "  - **C (Inverse of Regularization Strength)**:\n",
        "    - Controls the trade-off between achieving a low training error and a low testing error.\n",
        "    - Smaller C → Stronger regularization (simpler model).\n",
        "    - Larger C → Weaker regularization (more complex model).\n",
        "  - **Penalty**:\n",
        "    - Type of regularization to apply: `'l1'` (Lasso), `'l2'` (Ridge), or `'elasticnet'`.\n",
        "  - **Solver**:\n",
        "    - Algorithm used to optimize the cost function: `'liblinear'`, `'lbfgs'`, `'saga'`, etc.\n",
        "    - Not all solvers support all penalties (e.g., `'liblinear'` supports `'l1'` and `'l2'` only).\n",
        "  - **max_iter**:\n",
        "    - Maximum number of iterations taken for the solvers to converge.\n",
        "\n",
        "- **Why is Hyperparameter Tuning Important?**:\n",
        "  - It helps improve **model performance**, **reduce overfitting or underfitting**, and ensure the model generalizes well to unseen data.\n",
        "\n",
        "- **Techniques for Hyperparameter Tuning**:\n",
        "  - **Grid Search**:\n",
        "    - Tries all combinations of given hyperparameter values.\n",
        "    - Computationally expensive but exhaustive.\n",
        "  - **Random Search**:\n",
        "    - Samples random combinations of hyperparameters from specified distributions.\n",
        "    - More efficient when search space is large.\n",
        "  - **Bayesian Optimization**:\n",
        "    - Uses probabilistic models to predict good hyperparameter values based on past evaluation results.\n",
        "    - More sophisticated and often faster convergence.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "DINIQaqVyhzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "- **Solvers in Logistic Regression**:\n",
        "\n",
        "  A solver is an algorithm used to **optimize the cost function** in Logistic Regression. Different solvers use different strategies for optimization, and the choice depends on the dataset and regularization type.\n",
        "\n",
        "- **1. liblinear**:\n",
        "  - **Type**: Coordinate Descent Algorithm (good for small datasets).\n",
        "  - **Supports**: `'l1'` and `'l2'` penalties.\n",
        "  - **Doesn’t support**: Multinomial classification (`multi_class='multinomial'`).\n",
        "  - **Best For**: Small datasets, binary classification, sparse data.\n",
        "\n",
        "- **2. lbfgs (Limited-memory Broyden–Fletcher–Goldfarb–Shanno)**:\n",
        "  - **Type**: Quasi-Newton method.\n",
        "  - **Supports**: `'l2'` penalty only.\n",
        "  - **Supports Multiclass**: Yes (supports `multi_class='multinomial'`).\n",
        "  - **Best For**: Large datasets, multinomial classification.\n",
        "\n",
        "- **3. newton-cg**:\n",
        "  - **Type**: Newton-Raphson method.\n",
        "  - **Supports**: `'l2'` penalty.\n",
        "  - **Supports Multiclass**: Yes.\n",
        "  - **Best For**: Large datasets, multiclass problems, where higher accuracy is needed.\n",
        "\n",
        "- **4. sag (Stochastic Average Gradient)**:\n",
        "  - **Type**: Optimized for large datasets, especially when features are sparse.\n",
        "  - **Supports**: `'l2'` penalty only.\n",
        "  - **Requires**: Features to be scaled.\n",
        "  - **Best For**: Very large datasets with many samples.\n",
        "\n",
        "- **5. saga**:\n",
        "  - **Type**: Extension of SAG.\n",
        "  - **Supports**: `'l1'`, `'l2'`, and `'elasticnet'` penalties.\n",
        "  - **Supports Multiclass**: Yes.\n",
        "  - **Best For**: Large datasets, especially when using `l1` or `elasticnet` regularization.\n",
        "\n",
        "- **Which Solver to Use?**:\n",
        "\n",
        "| Scenario                                | Recommended Solver   |\n",
        "|----------------------------------------|-----------------------|\n",
        "| Small dataset, binary classification   | `liblinear`           |\n",
        "| Multiclass classification              | `lbfgs`, `saga`       |\n",
        "| L1 or Elastic Net regularization       | `liblinear`, `saga`   |\n",
        "| Very large datasets                    | `sag`, `saga`         |\n",
        "\n",
        "- **Note**: Always **scale your data** when using `sag` or `saga` for faster and stable convergence."
      ],
      "metadata": {
        "id": "SUD9GVQmN-IS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How is Logistic Regression extended for multiclass classification?\n",
        "- **Logistic Regression for Multiclass Classification**:\n",
        "\n",
        "  Although Logistic Regression is inherently designed for **binary classification**, it can be extended to handle **multiclass problems** using one of the following strategies:\n",
        "\n",
        "- **1. One-vs-Rest (OvR or One-vs-All)**:\n",
        "  - **Approach**: Builds **one binary classifier per class**.\n",
        "  - For each classifier:\n",
        "    - Treats one class as **positive** and the rest as **negative**.\n",
        "  - **Prediction**: Chooses the class with the **highest probability** among all classifiers.\n",
        "  - **Pros**: Simple and effective for most multiclass tasks.\n",
        "  - **Scikit-learn** default for `multi_class='ovr'`.\n",
        "\n",
        "- **2. Multinomial (Softmax Regression)**:\n",
        "  - **Approach**: Generalizes Logistic Regression to **predict probabilities for multiple classes** directly using the **softmax function**.\n",
        "  - **Softmax Function**:\n",
        "    $$\n",
        "    P(y = k \\mid x) = \\frac{e^{z_k}}{\\sum_{j=1}^K e^{z_j}}\n",
        "    $$\n",
        "    where $z_k = w_k^T x + b_k$, and $K$ is the number of classes.\n",
        "  - **Prediction**: Class with the **highest probability**.\n",
        "  - **Pros**: Often better calibrated probabilities and performance on multiclass problems.\n",
        "  - **Requires**: Solvers like `lbfgs`, `newton-cg`, or `saga`.\n",
        "  - **Scikit-learn**: Set `multi_class='multinomial'`.\n",
        "\n",
        "- **Comparison**:\n",
        "\n",
        "| Strategy        | Number of Models | Prediction Mechanism        | Use Case                            |\n",
        "|----------------|------------------|------------------------------|-------------------------------------|\n",
        "| One-vs-Rest     | K (K = classes)  | Choose highest predicted prob | Simpler, works well for many tasks |\n",
        "| Multinomial     | 1                | Softmax over all classes      | Preferred for probabilistic models |\n",
        "\n",
        "- **Important Note**:\n",
        "  - You must choose a solver that supports `multi_class='multinomial'` when using the softmax approach (e.g., `lbfgs`, `saga`).\n",
        "---"
      ],
      "metadata": {
        "id": "TzoYSTVEOVkw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "- **Advantages of Logistic Regression**:\n",
        "\n",
        "  - **1. Simple and Interpretable**: Easy to understand and explain; coefficients show the effect of each feature.\n",
        "  - **2. Fast and Efficient**: Works well with smaller datasets and converges quickly.\n",
        "  - **3. Probabilistic Output**: Predicts probabilities instead of just classes, which is helpful in decision-making.\n",
        "  - **4. Works Well with Linearly Separable Data**: Performs best when the classes are linearly separable.\n",
        "  - **5. Regularization Support**: Can include L1/L2 regularization to prevent overfitting.\n",
        "  - **6. Easy to Implement**: Widely available in most machine learning libraries (e.g., scikit-learn).\n",
        "\n",
        "- **Disadvantages of Logistic Regression**:\n",
        "\n",
        "  - **1. Assumes Linearity**: Assumes a linear relationship between input features and the log-odds of the output, which may not hold true in complex problems.\n",
        "  - **2. Not Suitable for Complex Relationships**: Struggles with non-linear patterns unless features are transformed.\n",
        "  - **3. Sensitive to Outliers**: Outliers can affect the model coefficients significantly.\n",
        "  - **4. Requires Feature Scaling**: Especially important when using regularization or solvers like `sag` and `saga`.\n",
        "  - **5. Poor Performance on Imbalanced Data**: Accuracy may be misleading if the classes are highly imbalanced.\n",
        "  - **6. Limited to Binary/Multiclass Classification**: Not suitable for regression or multi-label classification tasks.\n",
        "\n",
        "- **Summary Table**:\n",
        "\n",
        "| Aspect                  | Advantage                                      | Disadvantage                                   |\n",
        "|-------------------------|------------------------------------------------|------------------------------------------------|\n",
        "| Simplicity              | Easy to interpret                              | Limited flexibility for complex patterns       |\n",
        "| Speed                   | Fast training                                  | May oversimplify real-world relationships      |\n",
        "| Output                  | Probabilistic predictions                      | May require threshold tuning                   |\n",
        "| Model Assumptions       | Works well with linear separability            | Assumes linearity in log-odds                  |\n",
        "| Regularization Support  | Supports L1, L2, Elastic Net                   | Needs tuning of regularization parameters      |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "TRlmIR9XOgIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are some use cases of Logistic Regression?\n",
        "- **Use Cases of Logistic Regression**:\n",
        "\n",
        "  Logistic Regression is widely used in real-world applications that involve **binary or multiclass classification**. Its simplicity, interpretability, and probabilistic output make it ideal for many domains.\n",
        "\n",
        "- **1. Medical Diagnosis**:\n",
        "  - **Example**: Predicting whether a patient has a disease (Yes/No) based on symptoms and test results.\n",
        "  - **Why**: Helps in identifying high-risk patients for early intervention.\n",
        "\n",
        "- **2. Credit Scoring**:\n",
        "  - **Example**: Classifying whether a loan applicant is likely to **default** or not.\n",
        "  - **Why**: Used by banks and financial institutions for risk assessment.\n",
        "\n",
        "- **3. Email Spam Detection**:\n",
        "  - **Example**: Classifying emails as **spam or not spam**.\n",
        "  - **Why**: Simple yet effective approach in filtering unwanted emails.\n",
        "\n",
        "- **4. Customer Churn Prediction**:\n",
        "  - **Example**: Predicting if a customer will **leave a service** or not.\n",
        "  - **Why**: Helps companies take preventive actions to retain customers.\n",
        "\n",
        "- **5. Marketing Campaign Response**:\n",
        "  - **Example**: Predicting whether a customer will **respond to an ad** or campaign.\n",
        "  - **Why**: Useful in targeting the right audience and improving ROI.\n",
        "\n",
        "- **6. Fraud Detection**:\n",
        "  - **Example**: Detecting whether a transaction is **fraudulent** or genuine.\n",
        "  - **Why**: Critical for banks and payment gateways to prevent losses.\n",
        "\n",
        "- **7. Voting Behavior Prediction**:\n",
        "  - **Example**: Predicting whether a person will **vote for a candidate/party**.\n",
        "  - **Why**: Useful in political analysis and campaigning.\n",
        "\n",
        "- **8. HR Analytics**:\n",
        "  - **Example**: Predicting if an employee is **likely to resign**.\n",
        "  - **Why**: Helps in workforce planning and retention strategies.\n",
        "\n",
        "- **9. Sentiment Analysis**:\n",
        "  - **Example**: Classifying a review or comment as **positive or negative**.\n",
        "  - **Why**: Common in analyzing social media or product reviews.\n",
        "\n",
        "- **10. Insurance Underwriting**:\n",
        "  - **Example**: Estimating the probability of an **insurance claim** being filed.\n",
        "  - **Why**: Supports pricing and policy decision-making.\n",
        "\n",
        "- **Note**: Logistic Regression is preferred when **interpretability, speed, and probability estimation** are important.\n",
        "---"
      ],
      "metadata": {
        "id": "G4E5p35mOrxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "- **Logistic Regression**:\n",
        "  - **Purpose**: Used for **binary classification** (two classes: 0 or 1).\n",
        "  - **Output**: Predicts the **probability** of one class (usually class 1) using the **sigmoid function**.\n",
        "  - **Activation Function**:\n",
        "    $$\n",
        "    \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "    $$\n",
        "  - **Interpretation**: Output is a value between 0 and 1, representing the probability of the positive class.\n",
        "  - **Decision Rule**: Class label is assigned based on a threshold (commonly 0.5).\n",
        "\n",
        "- **Softmax Regression (Multinomial Logistic Regression)**:\n",
        "  - **Purpose**: Used for **multiclass classification** (more than two classes).\n",
        "  - **Output**: Predicts the **probability distribution** over all possible classes using the **softmax function**.\n",
        "  - **Activation Function**:\n",
        "    $$\n",
        "    \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} \\quad \\text{for } i = 1, 2, \\ldots, K\n",
        "    $$\n",
        "    where $K$ is the number of classes.\n",
        "  - **Interpretation**: Output is a vector of probabilities that sum to 1 — the model picks the class with the highest probability.\n",
        "\n",
        "- **Key Differences**:\n",
        "\n",
        "| Feature                 | Logistic Regression                    | Softmax Regression                          |\n",
        "|------------------------|----------------------------------------|---------------------------------------------|\n",
        "| Type of Problem         | Binary classification                  | Multiclass classification                   |\n",
        "| Output                  | Single probability                     | Probability distribution over all classes   |\n",
        "| Activation Function     | Sigmoid                                | Softmax                                     |\n",
        "| Classes Supported       | 2 classes                              | 3 or more classes                           |\n",
        "| Loss Function           | Binary Cross-Entropy (Log Loss)        | Categorical Cross-Entropy                   |\n",
        "\n",
        "- **Use Case Example**:\n",
        "  - **Logistic Regression**: Classify email as spam or not spam.\n",
        "  - **Softmax Regression**: Classify an image as cat, dog, or horse.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VPyDZMyOO32a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "- **One-vs-Rest (OvR)**:\n",
        "  - **Definition**: Trains **one binary classifier per class**.\n",
        "  - For $K$ classes, it builds **K logistic regression models**, each distinguishing one class from the rest.\n",
        "  - **Prediction**: The class whose classifier gives the **highest probability** is chosen.\n",
        "\n",
        "- **Softmax (Multinomial Logistic Regression)**:\n",
        "  - **Definition**: Trains **one model** that directly computes probabilities for **all classes simultaneously** using the softmax function.\n",
        "  - **Prediction**: The class with the **highest softmax score** is selected.\n",
        "\n",
        "- **Comparison**:\n",
        "\n",
        "| Feature                   | One-vs-Rest (OvR)                        | Softmax (Multinomial)                    |\n",
        "|---------------------------|------------------------------------------|------------------------------------------|\n",
        "| Number of Models          | One per class (K models)                 | One single model                         |\n",
        "| Complexity                | Simpler, easier to debug                 | Slightly more complex                    |\n",
        "| Training Time             | Slower (K models trained separately)     | Faster (one model trained)               |\n",
        "| Independence              | Assumes classes are **independent**      | Considers **mutual exclusivity**         |\n",
        "| Accuracy                  | Can be good when classes are unbalanced  | Often better with balanced data          |\n",
        "| Interpretability          | Easier to interpret per class            | Holistic but harder to interpret         |\n",
        "\n",
        "- **When to Use**:\n",
        "  - Use **OvR**:\n",
        "    - When classes are **not mutually exclusive**.\n",
        "    - For **highly imbalanced** or **sparse** classes.\n",
        "    - If you need **per-class control** or **interpretability**.\n",
        "  \n",
        "  - Use **Softmax**:\n",
        "    - When classes are **mutually exclusive**.\n",
        "    - For **balanced datasets** with **multiple classes**.\n",
        "    - When you want a **single probabilistic model**.\n",
        "\n",
        "- **Example**:\n",
        "  - **OvR**: Predicting if an article belongs to one or more topics (politics, sports, tech).\n",
        "  - **Softmax**: Predicting the digit (0-9) in handwritten digit recognition where only one correct class exists.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "fiZyToCuPDSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "- **Interpreting Coefficients in Logistic Regression**:\n",
        "\n",
        "  - In logistic regression, the **coefficients** (weights) represent the relationship between the **input features** and the **log-odds of the outcome**. The model predicts the probability of the positive class, and the coefficients help understand how each feature influences that prediction.\n",
        "\n",
        "- **Log-Odds Interpretation**:\n",
        "  - The model outputs the **log-odds** of the event (probability of class 1) based on the input features. The equation for logistic regression is:\n",
        "  \n",
        "  $$\n",
        "  \\log \\left( \\frac{P(y = 1 | X)}{1 - P(y = 1 | X)} \\right) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n\n",
        "  $$\n",
        "\n",
        "  where:\n",
        "  - $P(y = 1 | X)$ is the probability of class 1.\n",
        "  - $\\beta_0$ is the **intercept** (bias term).\n",
        "  - $\\beta_1, \\beta_2, \\ldots, \\beta_n$ are the **coefficients** corresponding to the features $x_1, x_2, \\ldots, x_n$.\n",
        "\n",
        "- **Exponentiating the Coefficients**:\n",
        "  - The coefficients can be interpreted in terms of **odds ratios** by exponentiating them. This gives a clearer understanding of how each feature affects the odds of the event happening.\n",
        "\n",
        "  $$\n",
        "  \\text{Odds Ratio} = e^{\\beta_i}\n",
        "  $$\n",
        "\n",
        "  - The **odds ratio** indicates the **change in the odds** for a **one-unit increase** in the corresponding feature.\n",
        "\n",
        "- **Interpretation of Coefficients**:\n",
        "  - **Positive Coefficient ($\\beta_i > 0$)**:\n",
        "    - If $\\beta_i$ is positive, the feature increases the odds of the positive class (class 1) as its value increases.\n",
        "    - Example: A positive coefficient for \"age\" might indicate that as age increases, the probability of the positive class (e.g., having a disease) increases.\n",
        "  \n",
        "  - **Negative Coefficient ($\\beta_i < 0$)**:\n",
        "    - If $\\beta_i$ is negative, the feature decreases the odds of the positive class as its value increases.\n",
        "    - Example: A negative coefficient for \"income\" might indicate that higher income decreases the likelihood of defaulting on a loan.\n",
        "\n",
        "- **Odds Ratio Interpretation**:\n",
        "  - **Odds Ratio > 1**: The feature increases the odds of the event occurring.\n",
        "  - **Odds Ratio = 1**: The feature has no effect on the odds.\n",
        "  - **Odds Ratio < 1**: The feature decreases the odds of the event occurring.\n",
        "\n",
        "- **Example**:\n",
        "  - Suppose we have the following logistic regression equation for predicting whether a person buys a product (1 = purchase, 0 = no purchase):\n",
        "  \n",
        "  $$\n",
        "  \\log \\left( \\frac{P(y = 1)}{1 - P(y = 1)} \\right) = -2 + 0.5 \\times \\text{Age} - 0.3 \\times \\text{Income}\n",
        "  $$\n",
        "\n",
        "  - **Intercept ($\\beta_0$)**: $-2$ – This is the log-odds of buying the product when Age and Income are both zero (which might not be realistic but serves as a baseline).\n",
        "  - **Coefficient of Age ($\\beta_1 = 0.5$)**: Each additional year of age increases the odds of purchasing the product by a factor of $e^{0.5} \\approx 1.65$.\n",
        "  - **Coefficient of Income ($\\beta_2 = -0.3$)**: For each additional unit of income, the odds of purchasing the product decrease by a factor of $e^{-0.3} \\approx 0.74$.\n",
        "\n",
        "- **Note**: Coefficients can be affected by **feature scaling** (e.g., standardization or normalization), so it's important to scale features if necessary to ensure proper interpretation.\n",
        "---"
      ],
      "metadata": {
        "id": "i0o4HvHpPNL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Practical Questions:"
      ],
      "metadata": {
        "id": "0QLBHAWqPZ40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2K62roKdPfJV",
        "outputId": "e69bc1c0-16f4-4ef7-b843-a21006b85f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy with L1 Regularization: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvLNs9V6QWEv",
        "outputId": "d9be5720-a5eb-4182-da96-bc5523825cc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 Regularization: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy with L2 Regularization: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the model coefficients\n",
        "print('Model Coefficients:')\n",
        "print(model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr7i5_F1ROGE",
        "outputId": "f26a3f9f-3b79-4970-e342-84751ece7777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L2 Regularization: 100.00%\n",
            "Model Coefficients:\n",
            "[[-0.39340204  0.96258576 -2.37510761 -0.99874603]\n",
            " [ 0.50840364 -0.25486503 -0.21301366 -0.77575487]\n",
            " [-0.1150016  -0.70772072  2.58812127  1.77450091]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with Elastic Net regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy with Elastic Net Regularization: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the model coefficients\n",
        "print('Model Coefficients:')\n",
        "print(model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-EFc3sSRT-v",
        "outputId": "d85f14f6-9185-411b-a262-557e2f486056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Elastic Net Regularization: 100.00%\n",
            "Model Coefficients:\n",
            "[[ 0.38798899  1.77424741 -2.42190524 -0.70704749]\n",
            " [ 0.07583151  0.          0.         -0.58136671]\n",
            " [-1.256549   -1.52800563  2.59413859  2.0815802 ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model for multiclass classification using OvR\n",
        "model = LogisticRegression(multi_class='ovr', max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy with One-vs-Rest (OvR): {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the model coefficients\n",
        "print('Model Coefficients:')\n",
        "print(model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxaoIDpRRsw4",
        "outputId": "9ec122af-ecdd-44d2-ea96-b7a156ca798c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with One-vs-Rest (OvR): 96.67%\n",
            "Model Coefficients:\n",
            "[[-0.42762216  0.88771927 -2.21471658 -0.91610036]\n",
            " [-0.03387836 -2.0442989   0.54266011 -1.0179372 ]\n",
            " [-0.38904645 -0.62147609  2.7762982   2.09067085]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Define a pipeline with scaling and logistic regression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(solver='liblinear'))  # liblinear supports both l1 and l2 penalties\n",
        "])\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'logreg__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'logreg__penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X, y)\n",
        "\n",
        "# Print best parameters and best accuracy\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n",
        "\n"
      ],
      "metadata": {
        "id": "KUUQIykfR3tS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e662d66-785f-40e1-cb67-01a54c146c08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'logreg__C': 0.1, 'logreg__penalty': 'l2'}\n",
            "Best Accuracy: 0.9824406148113647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print theaverage accuracy.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "logistic_model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "# Initialize Stratified K-Fold Cross-Validation\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation and obtain accuracy scores\n",
        "accuracy_scores = cross_val_score(logistic_model, X, y, cv=cv, scoring='accuracy')\n",
        "\n",
        "# Calculate the average accuracy\n",
        "average_accuracy = np.mean(accuracy_scores)\n",
        "\n",
        "# Print the accuracy scores for each fold\n",
        "print(\"Accuracy scores for each fold:\", accuracy_scores)\n",
        "\n",
        "# Print the average accuracy\n",
        "print(\"Average accuracy:\", average_accuracy)"
      ],
      "metadata": {
        "id": "0v0zuOf3VAnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e08969ab-6ab0-4042-abce-8b1616072c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy scores for each fold: [0.76 0.77 0.86 0.82 0.78 0.81 0.83 0.8  0.86 0.84]\n",
            "Average accuracy: 0.8130000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load CSV\n",
        "data = pd.read_csv('sample_data.csv')\n",
        "\n",
        "# Features and Target\n",
        "X = data[['Age', 'Salary']]\n",
        "y = data['Purchased']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train and predict\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "YHy5XvjaT4g4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b65fadaa-842a-43f5-cffb-155355ea766d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Pipeline with scaling and logistic regression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "# Define hyperparameter search space\n",
        "param_distributions = {\n",
        "    'logreg__C': uniform(loc=0.01, scale=10),  # Uniform distribution between 0.01 and 10.01\n",
        "    'logreg__penalty': ['l1', 'l2'],\n",
        "    'logreg__solver': ['liblinear', 'saga']  # Both support l1 and l2\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV setup\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=10,         # Number of combinations to try\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the search\n",
        "search.fit(X, y)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best Parameters:\", search.best_params_)\n",
        "print(\"Best Accuracy:\", search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x2r3h6gBbYR",
        "outputId": "f5b19214-50b9-4b61-86ce-a38de263c981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'logreg__C': np.float64(0.5908361216819946), 'logreg__penalty': 'l2', 'logreg__solver': 'liblinear'}\n",
            "Best Accuracy: 0.9824406148113647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load multiclass dataset (Iris)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Logistic Regression with One-vs-One strategy\n",
        "base_model = LogisticRegression(max_iter=1000)\n",
        "ovo_model = OneVsOneClassifier(base_model)\n",
        "\n",
        "# Train and predict\n",
        "ovo_model.fit(X_train, y_train)\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"One-vs-One Logistic Regression Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Aghi0ccBs_h",
        "outputId": "ae92c330-13c5-4f4d-98fe-6775995989c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Logistic Regression Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "labels = data.target_names\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Logistic Regression')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "o5l9Ib7zB2fg",
        "outputId": "25ddb5da-30d2-424c-e81a-0b5d6e40ff42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print scores\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GC_0sPACCCE",
        "outputId": "c562bd98-ad35-44a7-88ec-b4560f2b349b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9722\n",
            "Recall:    0.9859\n",
            "F1 Score:  0.9790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Create an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5,\n",
        "                           n_redundant=2, n_clusters_per_class=1,\n",
        "                           weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression without class_weight\n",
        "model_no_weight = LogisticRegression(max_iter=1000)\n",
        "model_no_weight.fit(X_train, y_train)\n",
        "y_pred_no_weight = model_no_weight.predict(X_test)\n",
        "f1_no_weight = f1_score(y_test, y_pred_no_weight)\n",
        "\n",
        "# Logistic Regression with class_weight='balanced'\n",
        "model_weighted = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = model_weighted.predict(X_test)\n",
        "f1_weighted = f1_score(y_test, y_pred_weighted)\n",
        "\n",
        "# Compare F1 scores\n",
        "print(f\"F1 Score without class_weight: {f1_no_weight:.4f}\")\n",
        "print(f\"F1 Score with class_weight='balanced': {f1_weighted:.4f}\")\n"
      ],
      "metadata": {
        "id": "pKC5SB-dCY_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f69ab655-eeba-4c2e-a698-e39625a3e8fe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score without class_weight: 0.9167\n",
            "F1 Score with class_weight='balanced': 0.8679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Handle missing values\n",
        "# Fill missing 'Age' with the median age\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "\n",
        "# Fill missing 'Embarked' with the most common port\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Drop the 'Cabin' column as it has too many missing values\n",
        "data.drop('Cabin', axis=1, inplace=True)\n",
        "\n",
        "# Convert categorical variables to numeric\n",
        "# Convert 'Sex' to numeric\n",
        "data['Sex'] = LabelEncoder().fit_transform(data['Sex'])\n",
        "\n",
        "# Convert 'Embarked' to numeric\n",
        "data['Embarked'] = LabelEncoder().fit_transform(data['Embarked'])\n",
        "\n",
        "# Select features and target variable\n",
        "X = data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]  # Features\n",
        "y = data['Survived']  # Target variable\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Display confusion matrix and classification report\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFe8it7Oma2E",
        "outputId": "d6e45461-bc22-4438-c24b-bbcd97ea47df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the dataset:\n",
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                           Allen, Mr. William Henry    male  35.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3      0            113803  53.1000  C123        S  \n",
            "4      0            373450   8.0500   NaN        S  \n",
            "Accuracy: 0.81\n",
            "Confusion Matrix:\n",
            "[[90 15]\n",
            " [19 55]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       105\n",
            "           1       0.79      0.74      0.76        74\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.81      0.80      0.80       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-6991c165cc24>:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['Age'].fillna(data['Age'].median(), inplace=True)\n",
            "<ipython-input-4-6991c165cc24>:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
        "data.drop('Cabin', axis=1, inplace=True)\n",
        "\n",
        "# Convert categorical variables to numeric\n",
        "data['Sex'] = LabelEncoder().fit_transform(data['Sex'])\n",
        "data['Embarked'] = LabelEncoder().fit_transform(data['Embarked'])\n",
        "\n",
        "# Select features and target variable\n",
        "X = data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]  # Features\n",
        "y = data['Survived']  # Target variable\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression without scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=200)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy without scaling\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling:.2f}\")\n",
        "\n",
        "# Logistic Regression with scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_with_scaling = LogisticRegression(max_iter=200)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate accuracy with scaling\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "print(f\"Accuracy with scaling: {accuracy_with_scaling:.2f}\")\n",
        "\n",
        "# Display confusion matrix and classification report for both models\n",
        "print(\"\\nConfusion Matrix without scaling:\")\n",
        "print(confusion_matrix(y_test, y_pred_no_scaling))\n",
        "print(\"\\nClassification Report without scaling:\")\n",
        "print(classification_report(y_test, y_pred_no_scaling))\n",
        "\n",
        "print(\"\\nConfusion Matrix with scaling:\")\n",
        "print(confusion_matrix(y_test, y_pred_with_scaling))\n",
        "print(\"\\nClassification Report with scaling:\")\n",
        "print(classification_report(y_test, y_pred_with_scaling))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgWNTKb-mlxj",
        "outputId": "066c61ba-47ba-4842-a585-9a7ce4ac3aac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-96a0494b8d64>:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['Age'].fillna(data['Age'].median(), inplace=True)\n",
            "<ipython-input-6-96a0494b8d64>:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.81\n",
            "Accuracy with scaling: 0.80\n",
            "\n",
            "Confusion Matrix without scaling:\n",
            "[[90 15]\n",
            " [19 55]]\n",
            "\n",
            "Classification Report without scaling:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       105\n",
            "           1       0.79      0.74      0.76        74\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.81      0.80      0.80       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n",
            "\n",
            "Confusion Matrix with scaling:\n",
            "[[90 15]\n",
            " [20 54]]\n",
            "\n",
            "Classification Report with scaling:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84       105\n",
            "           1       0.78      0.73      0.76        74\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.80      0.79      0.80       179\n",
            "weighted avg       0.80      0.80      0.80       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for ROC-AUC (use probability of class 1)\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTlzASZbnxPi",
        "outputId": "62628957-1ad0-4bce-d4b1-89135f25ee7c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create Logistic Regression model with custom learning rate (C=0.5)\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with C=0.5: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tisogxqfn_Xm",
        "outputId": "78e537b1-4e24-430a-df92-a5074d2f6a6b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split and scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get feature importance (coefficients)\n",
        "coefficients = model.coef_[0]\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': coefficients,\n",
        "    'Absolute Coef': np.abs(coefficients)\n",
        "})\n",
        "\n",
        "# Sort by absolute value of coefficients\n",
        "feature_importance = feature_importance.sort_values(by='Absolute Coef', ascending=False)\n",
        "\n",
        "# Display top important features\n",
        "print(\"Top important features based on Logistic Regression coefficients:\\n\")\n",
        "print(feature_importance[['Feature', 'Coefficient']].head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrGLknTjoHXf",
        "outputId": "4723dd3a-5b38-4626-bb67-98696cd14648"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top important features based on Logistic Regression coefficients:\n",
            "\n",
            "                 Feature  Coefficient\n",
            "21         worst texture    -1.350606\n",
            "10          radius error    -1.268178\n",
            "28        worst symmetry    -1.208200\n",
            "7    mean concave points    -1.119804\n",
            "26       worst concavity    -0.943053\n",
            "13            area error    -0.907186\n",
            "20          worst radius    -0.879840\n",
            "23            worst area    -0.841846\n",
            "6         mean concavity    -0.801458\n",
            "27  worst concave points    -0.778217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate using Cohen's Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CezLLSNjp99Y",
        "outputId": "c639197c-bab7-4efb-9560-efd95b4f1919"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.9437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision-recall values\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='blue', label=f'PR AUC = {pr_auc:.4f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "Zh9jQ8WBqGzE",
        "outputId": "9a80cc35-2f5b-4332-d326-8ae1c3ef0bf4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Solvers to test\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "accuracy_scores = {}\n",
        "\n",
        "# Train and evaluate models with different solvers\n",
        "for solver in solvers:\n",
        "    try:\n",
        "        model = LogisticRegression(solver=solver, max_iter=1000)\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        accuracy_scores[solver] = accuracy\n",
        "    except Exception as e:\n",
        "        accuracy_scores[solver] = f\"Error: {e}\"\n",
        "\n",
        "# Print comparison results\n",
        "print(\"Accuracy comparison of Logistic Regression with different solvers:\\n\")\n",
        "for solver, acc in accuracy_scores.items():\n",
        "    print(f\"{solver}: {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8egaKX7irgRe",
        "outputId": "739cf908-b428-4fe3-b0a2-ae9da12e667a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy comparison of Logistic Regression with different solvers:\n",
            "\n",
            "liblinear: 0.9736842105263158\n",
            "saga: 0.9736842105263158\n",
            "lbfgs: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate using Matthews Correlation Coefficient\n",
        "mcc_score = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8vGs7hMtmM1",
        "outputId": "201bee30-e36a-4c1e-f1b0-d42179bab4d4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.9439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
        "data.drop('Cabin', axis=1, inplace=True)\n",
        "\n",
        "# Convert categorical variables to numeric\n",
        "data['Sex'] = LabelEncoder().fit_transform(data['Sex'])\n",
        "data['Embarked'] = LabelEncoder().fit_transform(data['Embarked'])\n",
        "\n",
        "# Select features and target variable\n",
        "X = data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]  # Features\n",
        "y = data['Survived']  # Target variable\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression on raw data\n",
        "model_raw = LogisticRegression(max_iter=200)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy on raw data\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "print(f\"Accuracy on raw data: {accuracy_raw:.2f}\")\n",
        "\n",
        "# Logistic Regression on standardized data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate accuracy on standardized data\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy on standardized data: {accuracy_scaled:.2f}\")\n",
        "\n",
        "# Display confusion matrix and classification report for both models\n",
        "print(\"\\nConfusion Matrix on raw data:\")\n",
        "print(confusion_matrix(y_test, y_pred_raw))\n",
        "print(\"\\nClassification Report on raw data:\")\n",
        "print(classification_report(y_test, y_pred_raw))\n",
        "\n",
        "print(\"\\nConfusion Matrix on standardized data:\")\n",
        "print(confusion_matrix(y_test, y_pred_scaled))\n",
        "print(\"\\nClassification Report on standardized data:\")\n",
        "print(classification_report(y_test, y_pred_scaled))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c5sRNmEuYQq",
        "outputId": "f10014ff-e129-4b3c-cca2-2b1ca88f95d9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data: 0.81\n",
            "Accuracy on standardized data: 0.80\n",
            "\n",
            "Confusion Matrix on raw data:\n",
            "[[90 15]\n",
            " [19 55]]\n",
            "\n",
            "Classification Report on raw data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       105\n",
            "           1       0.79      0.74      0.76        74\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.81      0.80      0.80       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n",
            "\n",
            "Confusion Matrix on standardized data:\n",
            "[[90 15]\n",
            " [20 54]]\n",
            "\n",
            "Classification Report on standardized data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84       105\n",
            "           1       0.78      0.73      0.76        74\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.80      0.79      0.80       179\n",
            "weighted avg       0.80      0.80      0.80       179\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-d868587d0035>:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['Age'].fillna(data['Age'].median(), inplace=True)\n",
            "<ipython-input-15-d868587d0035>:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Grid of C values to try\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "}\n",
        "\n",
        "# Grid search with cross-validation\n",
        "grid = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best C value\n",
        "best_C = grid.best_params_['C']\n",
        "best_score = grid.best_score_\n",
        "\n",
        "# Evaluate on test set with best C\n",
        "final_model = LogisticRegression(C=best_C, max_iter=1000)\n",
        "final_model.fit(X_train_scaled, y_train)\n",
        "y_pred = final_model.predict(X_test_scaled)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Results\n",
        "print(f\"Best C from cross-validation: {best_C}\")\n",
        "print(f\"Cross-validation accuracy:    {best_score:.4f}\")\n",
        "print(f\"Test set accuracy:            {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjs1m44EvlkN",
        "outputId": "f7b5adcc-acc0-46f3-ef96-067560809565"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C from cross-validation: 10\n",
            "Cross-validation accuracy:    0.9758\n",
            "Test set accuracy:            0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Save the model and scaler using joblib\n",
        "joblib.dump(model, \"logistic_model.pkl\")\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "# Load the model and scaler\n",
        "loaded_model = joblib.load(\"logistic_model.pkl\")\n",
        "loaded_scaler = joblib.load(\"scaler.pkl\")\n",
        "\n",
        "# Transform test data using the loaded scaler\n",
        "X_test_scaled_loaded = loaded_scaler.transform(X_test)\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "y_pred = loaded_model.predict(X_test_scaled_loaded)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy using loaded model: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLjtny7-xF-h",
        "outputId": "015bc980-c7a1-48ef-aeed-d76a9d70e4a4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using loaded model: 0.9737\n"
          ]
        }
      ]
    }
  ]
}